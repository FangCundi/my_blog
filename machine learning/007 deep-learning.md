# 深度学习

## base

流程图：通过传感器获得数据，经过预处理、特征提取、特征选择({$x_i,y_i$})、再到推理、预测或识别
机器学习关注识别的过程
特征表达：预处理、特征提取、特征选择，一般人工，相当于打标签，得到数据集

深度学习：自动的学习特征，也叫<b>表示学习</b>，是一个不断迭代、不断抽象的过程
信息处理是分级的，倾向性不断增加，语义信息越来越明显
高层特征是低层特征的组合，从底层到高层特征越来越抽象，越来越能表现语义或意图
抽象越高，可能猜测越少，越利于分类

## 神经网络

网络拓扑结构(连接方式)
相当于通过拓扑结构定义了假设空间中的函数是如何选择的
不同拓扑，则假说集不同，函数结构不同
神经元权重由数据决定
①前馈神经网络：多层感知机
②反馈神经网络：网络内神经元间有反馈，信息处理是状态交换

## 神经元

人工神经元用一个非线性激活函数，输出一个活性值a
d维输入$x=[x_1,...,x_d]^T$，有
$z=w^Tx+b,a=f(z)$
$w$是一个d维度的权重向量，b是偏置量，z为状态，表示x输入的加权和，f是激活函数，如sigmod

对神经元组合后，即符合运算，问题是如何找权重(数据调)
修正线性单元：用rectifer函数(rectifier(x)=max(0,x))的单元，有单侧已知、宽兴奋边界、稀疏激活性
sigmod函数是一类S型曲线函数
logistics函数$\sigma(x)=\frac{1}{1+e^{-x}},\sigma'(x)=(1-\sigma(x))\sigma(x)$
tanh函数$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$

前馈网络参数：
$L$：网络层数
$n^l$：第l层神经元个数
$f_l()$：第l层激活函数
$w^l\in R^{n^l*n^{l-1}}$：l-1与l层的权重矩阵
$b^l\in R^{n^l}$：第l层的偏置量
$z^l\in R^l$：第l层的状态
$a^l\in R^l$：第l层的活性值
可有
<font color='red'><font size=5>
$z^l=w^{lT}a^{l-1}+b^l,a^l=f_l(z^l)$
</font></font>
输入是上一层的输出
也可以合并写为
$z^l=w^{lT}f_{l-1}(z^{l-1})+b^l$
即每一层的信息为
$x=a^0\rightarrow z^1\rightarrow a^1\rightarrow z^2\rightarrow...\rightarrow a^{L-1}\rightarrow z^{L}\rightarrow a^{L}=y$

样本$\{(x_i,y_i)\}_{i=1}^n$，输出为$f(x|w,b)$
目标：$J(w,b)=\sum_{i=1}^n l(f(x_i|w,b),y_i)+\frac{\lambda}{2}|w|_F^2=\sum_{i=1}^n J(w,b,x_i,y_i)+\frac{\lambda}{2}|w|_F^2$
其中$l$是训练误差，$\frac{\lambda}{2}|w|_F^2$表示正则化学习，正则化项，用来剪枝
其中$|w|_F^2=\sum_{l=1}^L\sum_{i=1}^{n^{l+1}}\sum_{j=1}^{n^l}(w_{ij}^l)^2$

要求最小化$J(w,b)$，采用梯度下降的方法，可有
$w^l=w^l-\alpha(\frac{d J(w,b)}{dw^l})=w^l-\alpha(\sum_{i=1}^n(\frac{dJ(w,b,x_i,y_i)}{dw^l})+\lambda w^l)$
同理可有
$b^l=b^l-\alpha(\frac{d J(w,b)}{db^l})=b^l-\alpha(\sum_{i=1}^n(\frac{dJ(w,b,x_i,y_i)}{db^l}))$
$\alpha$是参数的更新(学习)率，即步长
$\frac{dT(w,b)}{dw^l}$是方向

根据链式法则，有
$\frac{dJ(w,b,x,y)}{dw_{ij}^l}=(\frac{dJ(w,b,x,y)}{dz^l})^T\frac{dz^l}{dw_{ij}^l}$

则对于第$l$层，定义一个误差项$\sigma^l=\frac{dJ(w,b,x,y)}{dz^l}\in R^{n^l}$为目标函数关于第$l$层的神经元$z^l$的偏导数，即第$l$层神经元对最终误差的影响
因为$z^l=w^{lT}a^{l-1}+b^l$
则有$\frac{dz^l}{dw_{ij}^l}=\frac{d(w^la^{l-1}+b^l)}{dw_{ij}^l}=\left[ \begin{array}{} 0\\...\\a_j^{l-1}\\...\\0 \end{array} \right]$
故有$\frac{dJ(w,b,x,y)}{dw_{ij}^l}=\sigma_i^la_j^{l-1}$
故有$\frac{dJ(w,b,x,y)}{dw^l}=\sigma^l(a^{l-1})^T$
同理$\frac{dJ(w,b,x,y)}{db^l}=\sigma^l$
则第$l$层的误差项为
$\sigma^l=\frac{dJ(w,b,x,y)}{dz^l}=\frac{d a^l}{d z^l}\frac{dz^{l+1}}{d a^l}\frac{dJ(w,b,x,y)}{dz^{l+1}}$
代入激活函数后有
$\sigma^l=diag(f_l'(z^l))(w^{l+1})^T\sigma(l+1)$
可以定义向量点积，表示每个元素相乘，就有
$\sigma^l=f_l'(z^l)\odot ((w^{l+1})^T\sigma^{l+1})$
可以看到，第$l$层的误差可由第$l+1$得到，相当于错误反向传递，可由后层的误差计算前层的误差
由每一层的误差，可得每一层参数的梯度

训练过程：

1. 先前馈样本计算每一层的$z$与$a$
2. 反向传播$\sigma^l$计算每一层的$\sigma^l$
3. 通过$\sigma^l$计算每一层的偏导，更新参数$w,b$

## 卷积神经网络

局部连接、权重共享，空间或事件上的次采样
平移、缩放、扭曲不变形

卷积：给定输入序列$x_t(t=1,...,n)$，滤波器$\sigma_k(k=1,...,m)$(移动窗口)，输出$y_t=\sum_{k=1}^mf_kx_{t-k+1}$
<font color='red'><b>实际上就是一个窗口内的加权求和</b></font>
当$f_k=\frac{1}{m}$时，卷积相当于信号序列的移动平均
宽卷积：输出n+m-1，对不在$[1,n]$内的$x_t$补零
窄卷积：输出n-m+1，不补零
二维：图像$x_{ij}(1<i<m,1<j<n)$，滤波器$f_{ij}(1<i<m,1<j<n)$
即$y_{ij}=\sum_{u=1}^m\sum_{v=1}^nf_{uv}x_{i-u+1,j-v+1}$
用卷积取代替全连接，第$l$层每个神经元只与第$l-1$层的一个局部窗口内的神经元相连，局部连接

则第$l$层的第$i$个神经元的输入定义为：
$a_i^l=f_l(\sum_{j=1}^mw_j^{lT}a_{i-m+j}^{l-1}+b^l)=f_l(w^{lT}a_{i-m+1:i}^{l-1}+b^l)$
其中$w^l\in R^m$为$m$维滤波器，$a_{i-m+1:i}^{l-1}=[a_{i-m+1}^{l-1},...,a_i^{l-1}]^T$
也可写为$a^{l}=f_l(w^l\otimes a^{l-1}+b^l)$
其中$\otimes$表示卷积运算

<b>可以看到，$w^l$对于所有的神经元都是相同的，可以理解为一层只有一个卷积核</b>
在图像处理中，需要二维矩阵的形式输入到神经网络中，因此需要二维卷积，假设$x^l\in R^{w_l*h_l}$和$x^{l-1}\in R^{w_{l-1}*h_{l-1}}$分别表示第$l$层和$l-1$层的神经元活性值，其中$x^l$的每一个元素为
$x_{s,t}^{l}=f_l(\sum_{i=1}^{u}\sum_{j=1}^{v}w_{ij}^lx_{s-i+u,t-j+v}^{l-1}+b^l)$
其中$w^l\in R^{u*v}$为两维滤波器，$b^l$为第$l$层偏置，第$l$层的神经元个数为$w_l*h_l$，并且$w_l=w_{l-1}-u+1,h_{l}=h_{l-1}-v+1$
可有
$x^l=f_l(w^l\otimes x^{l-1}+b^l)$

但对每一层，有多组卷积核：$x^{l,k}=f_l(\sum_{p=1}^{n_l-1}(w^{l,k,p}\otimes x^{l-1,p})+b^{l,k})$
其中$w^{l,k,p}$表示第$l-1$层的第$p$组特征向量到第$l$层的第$k$组特征映射所需要的滤波器，即卷积核

连接表：第$l$层的每一组特征映射依赖$l-1$层的所有特征映射，即不同层的特征映射之间是全连接，但这种全连接关系不是必须的，可用表来表示怎么连接，即第$l$层的每一组特征映射都依赖于前一层的少数几组特征映射

池化层：pooling，即子采样(sub-sampling)，有max/min/aver三种，用max/min/aver代替某一区域的值，降低维度，减少神经元个数

### 例 输入为32*32*1的灰度图片

|输入32*32|核大小均为6*5*5，即6个|池化2*2*1|补充|
|--|--|--|--|
|第一层卷积|$32*32*1\rightarrow 28*28*6$|32-5+1=28||
|第二层池化|$28*28*6\rightarrow 14*14*6$|28/2=14||
|第三层卷积|$14*14*6\rightarrow 10*10*16$|14-5+1=10|有连接表，不再全连接|
|第四层池化|$10*10*16\rightarrow 5*5*16$|10/2=5||
|第五层卷积|$5*5*16\rightarrow 1*1*120$|5-5+1=1|全连接，120个核，16相加得1|
|第六层卷积|$1*1*120\rightarrow 1*1*84$|人为设置84|全连接|
|第七层卷积|$1*1*84\rightarrow 1*1*10$|全连接|输出层10个欧式径向基函数|

## GAN生成对抗网络

概率生成模型，学习概率分布(数据)采样得到新的数据
生成器，判别器
$\left. \begin{array}{} 真实的图像\rightarrow采样得到X-P_{data}\\隐随机变量(噪声)z-P_z\rightarrow生成网络G\rightarrow采样得到G(z)-Q \end{array} \right\}判别网络D\rightarrow P$
判定为假时更新生成网络
判定为真时更新判断网络
个人理解：这里可以理解为人为认为所有的生成结果都是错的，所以如果能够判别出来是假的，相当于是真值，要生成更好的；如果判别是真的，相当于是假值，需要修正判定网络
最后会平衡
目标为：$\min_G\max_DV(D,G)=E_{x-P_{data}}(x)[log D(x)]+E_{z-P_{z}(z)}[log(1-D(G(z)))]$
其中$D(x)$表示$x$来自真实样本的概率
