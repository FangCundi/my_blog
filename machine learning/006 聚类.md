# 聚类分析

## base

是一种无监督学习，给数据集，自动分为k个簇，使同一个簇中的样本尽可能相同
可以根据距离定义相似度，有多种距离
<b>可用于图形分割</b>

## 层次聚类

自底向上或自顶向下

自底向上：每一次从样本集中抽出两个簇进行合并(相似度最好)，结合成一个簇，簇数从样本数依次递减，直到达到目标簇数(需求)
难点是如何计算两个簇之间的相似度，以及如何快速计算最相似的簇
从$n*n$到$n-1*n-1$时，有$n-2*n-2$个值不会发生改变，所以可以加一行一列，扔掉之前合并的两行两列，即单独进行计算即可

自顶向下：一开始所有的样本属于一个簇，每次都有一个簇从当前样本中分出去，难点是如何选择分割簇的大小

簇相似度：最小距离、最大距离、平均距离

## k均值

给n个样本点，k个簇个数
求k个均值，每个均值指代每个簇，即中心。每个样本找最小距离的中心，属于那个簇
得到聚类之后再重新计算中心，用样本均值代替k均值，再重新划分，交替执行：
$初始化\mu_k\rightarrow划分x\rightarrow计算样本均值\overline{x_k}\rightarrow均值更新\mu_k$
$\mu_k=\frac{1}{|C_k|}\sum_{j\in C_k}x_j$即样本均值
其中$C_k$代表第k组的样本个数

### 求证：是否收敛

$\mu:\mu_1,...,\mu_k$，k均值
定义$r_{ik}\in\{0,1\}$，表示$x_i$是否属于$k$簇
目标：最小化$J(\mu,r)=\sum_{i=1}^n\sum_{k=1}^Kr_{ik}|x_i-\mu_{k}|^2$
要求$r_i=[r_{i1},...,r_{ik}]^T,r_{ik}\in \{0,1\},r_i^T*1=1,\forall i$即要求每行(列)求和=1，$\sum_{k=1}^Kr_{ik}=1$
调整$\mu,r$使$J(\mu,r)$最小，由于交替更新$\mu,r$，所以要固定一个

①给定$\mu$，优化$r,r\in\{r_1,...,r_k\},r_i^T1=1,\forall i,r_i,r_j$不存在关联，可拆分
这里相当于固定k个均值，而去调整优化每个样本的簇分类，相当于上文中的<b>划分</b>过程
$\forall i,\min_{r_i}\Longrightarrow\sum_{k=1}^Kr_{ik}|x_i-\mu_k|^2,s.t.\sum_{k=1}^Kr_{ik}=1,r_{ik}\in\{0,1\}$
即要求每个样本被划分到距离他最近的均值中
令$d_{ik}=|x_i-\mu_k|^2,d_i=\left( \begin{array}{} d_{i1}\\.\\.\\.\\d_{iK} \end{array} \right)$
相当于找到了最小的$d_{ik}$就确定了$r_{ik}$的序列

②给定$r$，优化$\mu,\forall i\min_{\mu_i}\sum_{i=1}^n\sum_{k=1}^Kr_{ik}|x_i-\mu_k|^2$
优化目标不变，相当于固定样本的簇分类不变，而去调整均值，相当于上文中的<b>均值更新</b>过程
直接求导可有
$\frac{dJ}{d\mu_{k}}=\sum_{i=1}^nr_{ik}(2x_i-2\mu_k)=0$
这里注意，是针对某一个$\mu_k$求导，相当于会有K个式子
可有$\sum_{i=1}^nr_{ik}x_i=\sum_{i=1}^n\mu_kr_{ik}$
即$\mu_k=\frac{\sum_{i=1}^nr_{ik}x_i}{\sum_{i=1}^nr_{ik}}$
<font color='red'><b>即分子是属于第k簇的样本之和，分母是属于第k簇的样本个数</font></b>
对每个$\mu_k$都要进行更新，就相当于用样本均值$\overline{x}$代替$\mu_k$

①②算一轮迭代，交替多次，相当于阶梯型前进
假设t轮时，有$\left. \begin{array}{} J(\mu_t,r_t)&证明J(\mu_t,r_t)\ge J(\mu_t,r_{t+1})\\J(\mu_t,r_t)&证明J(\mu_t,r_{t+1})\ge J(\mu_{t+1},r_{t+1}) \end{array} \right.$
即可证明经过一次迭代，目标在下降

还要证明有下界，即样本均值最终会和k均值重合

k均值初始化的时候：第一个$\mu$随机，第二个选择距离现有$\mu$最远的样本点，加松弛的时候，就进行软划分，使$r_{ik}\in(0,1)$

## 谱聚类

<b>谱：矩阵的最大特征值</b>
将样本看做高维图中的点，边看做样本的相似度(权重)，可有$graph:G(V,E,W)$
其中$w_{ij}=\exp\{-\frac{|x_i-x_j|^2}{2\sigma^2}\}$
令$cut(A,B)$表示分割簇A与簇B所需要切断的边的权重

<font color='red'><b>
目标：$\min cut(A,B)=\sum_{i\in A,i\in B}w_{ij}$
此时为二分类问题
</font></b>
化为了最小生成树问题
为了简便，化为有加权的最小分割
$Ncut(A,B)=cut(A,B)(\frac{1}{vol(A)}+\frac{1}{vol(B)})$
这一步是为了让A,B中元素的个数的差距不要太大
其中$vol(A)=\sum_{i\in A}d_i,d_i=\sum_{j=1}^nw_{ij}$
即$d_i$表示第$i$行的行和，可以理解为点$i$与其他所有样本点的权重之和
而$vol(A)$则表示同属于A类的点距离除他之外所有点的权重之和的和
当然，也可以直接用$|A|$代替$vol(A)$

目标：$\min Ncut(A,B)=cut(A,B)(\frac{1}{vol(A)}+\frac{1}{vol(B)})$

为了化简计算，定义$f=[f_1,...,f_n]^T,f_i=\left\{ \begin{array}{} \frac{1}{vol(A)}&if.i\in A\\-\frac{1}{vol(B)}&if.i\in B \end{array} \right.$
可以理解为将$r_i$的信息与权重结合起来成了$f_i$，用正负来区分$A,B$类
令$w1_n=求行和=列向量=w_i=\left( \begin{array}{} d_1\\.\\.\\.\\d_n \end{array} \right)$
$D=diag(w1_n)$，即将$w1_n$扩展为对角矩阵$D=\left( \begin{array}{} d_1,0,...,0\\0,d_2,...,0\\.,.,...,.\\0,0,...,d_n \end{array} \right)$
$L=D-W$
则可有
$f^TLf=\sum_{ij}w_{ij}(f_i-f_j)^2$，即求列和、再求行和，两次求和
根据求和定义可有
$f^TLf=\sum_{ij}f_if_jL_{ij}$
将$L=D-W$代入可有
$f^TLf=\sum_{ij}f_if_j(D_{ij}-w_{ij})=f^T(D-w)f$
展开有
$f^TLf=f^TDf-f^Twf$
因为$d_i=\sum_{j=1}^nw_{ij}$
所以可有
$f^TLf=\sum_{i=1}^nd_if_i^2-f^Twf$
故可有
$f^TLf=\sum_{i=1}^nd_if_i^2-\sum_{i=1}^n\sum_{j=1}^nf_if_jw_{ij}$
将$\sum_{i=1}^n$拆开，为$\frac{1}{2}\sum_{i=1}^n+\frac{1}{2}\sum_{j=1}^n$
故可以有
$f^TLf=\frac{1}{2}\sum_{i=1}^nd_if_i^2+\frac{1}{2}\sum_{j=1}^nd_jf_j^2-\sum_{i=1}^n\sum_{j=1}^nf_if_jw_{ij}$
由于$w_{ij}$是对称的，有$w_{ij}=w_{ji}$
故
$f^TLf=\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^nw_{ij}f_i^2+\frac{1}{2}\sum_{j=1}^n\sum_{i=1}^nw_{ji}f_j^2-\sum_{i=1}^n\sum_{j=1}^nf_if_jw_{ij}$
可有
$f^TLf=\sum_{i=1}^n\sum_{j=1}^n(\frac{1}{2}w_{ij}f_i^2+\frac{1}{2}w_{ij}f_j^2-f_if_jw_{ij})$
可见，这为一个完全平方式
$f^TLf=\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^nw_{ij}(f_i-f_j)^2$
其中$f_i,f_j$是标量
由于只有当$i,j$不同簇的时候$f_i-f_j$不为0，有意义，故可有
$f^TLf=\frac{1}{2}(\sum_{i\in A}\sum_{j\in B}w_{ij}(\frac{1}{vol(A)}+\frac{1}{vol(B)})^2+\sum_{i\in B}\sum_{j\in A}w_{ij}(\frac{1}{vol(A)}+\frac{1}{vol(B)})^2)$
由于$i,j$是等价的，所以可以合并为
$f^TLf=\sum_{i\in A}\sum_{j\in B}w_{ij}(\frac{1}{vol(A)}+\frac{1}{vol(B)})^2$
故
$f^TLf=\sum_{i\in A,j\in B}w_{ij}(\frac{1}{vol(A)}+\frac{1}{vol(B)})
^2$

而考虑$f^TDf$，有
$f^TDf=\sum_{i=1}^nd_if_i^2=\sum_{i\in A}\frac{d_i}{vol(A)^2}+\sum_{i\in B}\frac{d_i}{vol(B)^2}$
又因为$\sum_{i\in A}d_i=\frac{1}{vol(A)}$
故可化简为
$f^TDf=\frac{1}{vol(A)}+\frac{1}{vol(B)}$

故可以有
$\frac{f^TLf}{f^TDf}=\sum_{i\in A,j\in B}w_{ij}(\frac{1}{vol(A)}+\frac{1}{vol(B)})=Ncut(A,B)$
<b>
相当于将目标转化为了求一个$f$，使得$\min_fNcut(A,B)=\frac{f^TLf}{f^TDf}$
</b>
而对应一个$f$都有一个划分向量$r$，但由于$f$是连续的，而$r$是离散的，需要想个办法将松弛转为连续
由于
$f^TD1=\sum_{i=1}^nf_id_i=\sum_{i\in A\cup B}f_id_i=\sum_{i\in A}f_id_i+\sum_{i\in B}f_id_i=\sum_{i\in A}\frac{d_i}{vol(A)}-\sum_{i\in B}\frac{-d_i}{vol(B)}=1+(-1)=0$
故在离散的时候，由于$f$只有$\frac{1}{vol(A)}$和$\frac{-1}{vol(B)}$两个值，所以默认了$f^TD1=0$成立
但当我们需要进行优化的时候，需要对f进行松弛，$f$可取$(\frac{-1}{vol(B)},\frac{1}{vol(A)})$中的任何值，所以需要强制要求$f^TD1=0$成立，相当于有一个约束条件
<font color='red'><b>
故优化目标转化为$\min_f\frac{f^TLf}{f^TDf},s.t.f^TD1=0$
</font></b>
在求解的时候，先不考虑约束，直接求导计算一个值，然后看在该值的情况下是否会满足约束条件即可
$\frac{d(\frac{f^TLf}{f^TDf})}{df}=\frac{2Lff^TDf-f^TLf2Df}{(f^TDf)^2}$
要求导数等于0，且由于$\frac{d(\frac{1}{2}x^TAx)}{dx}=Ax$，可有
$2Lff^TDf-f^TLf2Df=0$
可有
$Lff^TDf=f^TLfDf$
可有
$Lf=\frac{f^TLf}{f^TDf}Df$
令$\lambda=\frac{f^TLf}{f^TDf}$，可有
<font color='red'>
$Lf=\lambda Df$
</font>
这就是一个特征方程$D^{-1}Lf=\lambda f$
其中$\lambda$是个标量，即为特征值，$f$是特征向量
即$\frac{f^TLf}{f^TDf}$最优应满足$D^{-1}Lf=\lambda f$

先求证此时约束条件$f^TD1=0$是否依然满足
有$Lf=\lambda Df$，两边转置有$\lambda f^TD^T=f^TL^T$
由于$D,L$均对称，故有$\lambda f^TD=f^TL$
即
$\lambda f^TD1=f^TL1=f^T(D-w)1$
可有
$\lambda f^TD1=f^T(D1-w1)$
因为$D1-w1=0$求行和，即$L1=0$
故可有$\lambda f^TD1=0$
又因为$\lambda \ne 0$，否则根据$\lambda=\frac{f^TLf}{f^TDf}$，有$f^TLf=0,f=1$，没有意义
所以有
$Lf=\lambda Df$时，$f^TD1=0$也成立，自动满足

所以这个结果可以用，取$\lambda$是一个非零的最小特征值对应的特征向量$f$作为分类依据，对$f$离散化，根据$>0,<0$，可有分类$r$

## 谱聚类k个簇

求$Lf=\lambda Df$的前k个最小特征值对应的特征向量(此时可以选0，没有影响，因为0是均分)
可以得到一个$V\in R^{n*k}$的矩阵
$$
V=\left( \begin{array}{}
V&v_1&v_2&...&v_k\\
z_1&v_{11}&v_{12}&...&v_{1k}\\
.&.&.&...&.\\
z_n&v_{n1}&v_{n2}&...&v_{nk}
\end{array} \right)
$$
其中每一列为一个特征向量，每一行是一个样本点采用不同的k个分类器分类的结果，相当于<b>k次二分类结果矩阵</b>
将这个$n*k$矩阵作为$k$均值的输入，可以得到一个$k$均值的解，作为样本分类情况。
相当于$n*n\rightarrow n*k$或$n*m\rightarrow n*k$，类似于降维，将数据进行了抽象
