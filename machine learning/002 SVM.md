# 支持向量机与核算法SVM

## base

输入空间：$X\subseteq R^d$
输出空间（标签）：$Y=\{-1,+1\}$
未知的目标函数：$f:X\rightarrow Y$
训练集合：$S=\{(x_i,y_i)\}_{i=1}^n,y_i=f(x_i),\{x_i\}_{i=1}^n$从输入空间X中依据分布D采样得到，n为样本个数
假说集合：$H=\{x\rightarrow sign(w^Tx+b):w\in R^d,b\in R\}$
假设h是某个具体假设函数
经验误差：$R_S(h)=\frac 1n \sum_{i=1}^n[h(x_i)\ne f(x_i)]$ with $h\in H$，表示分类错误，是针对训练集的误分率
泛化误差：$R_D(h)=P_{x\in D}[h(x)\ne f(x)]$with$h\in H$，表示针对整个样本分布，错误分类的概率，类似于噪声，是在新样本集合上的错分率
分类问题：
&emsp;&emsp;给定样本$S=\{(x_1,y_1),...(x_n,y_n)\},\{x_i\}_{i=1}^n\subseteq X^n$独立同分布，$y_i=f(x_i)\in Y$
二分类问题基于数据S，从假说集合H中选择一个h，期望误差：
&emsp;&emsp;$E_{x-D}[h(x)\ne f(x)]=1*P_{x-D}[h(x)\ne f(x)]+0*P(x-D)[h(x)=f(x)]$，1表示错误分类代价

## SVM

考虑超平面$w^Tx+b=0$
给定$a>0$，要求其平面对正样本：$y=1,w^Tx+b\ge a$；对负样本：$y=-1,w^Tx+b\le -a$
样本$x_i$到超平面$w^T+b=0$的距离为：$\frac{|w^Tx_i+b|}{|w|}=\frac{y_i(w^Tx_i+b)}{|w|}$
样本集S到超平面$w^T+b=0$的距离$\rho$为：$\rho=\min_{(x_i,y_i)}=\frac{y_i(w^Tx_i+b)}{|w|}=\frac{a}{|w|}$，即距离超平面最近的点的距离
故优化目标为$\max_{w,b}\frac{a}{|w|},s.t.y_i(w^Tx_i+b)\ge a,\forall i$，即使距离超平面的点与超平面的距离a最大

### 线性可分

优化目标为$\max_{w,b}\frac{a}{|w|},s.t.y_i(w^Tx_i+b)\ge a,\forall i$
其中a未知，正样本$w^T+b\ge a$，负样本$w^T+b\le -a$
即要最近的点的距离最大化$y_i(w^Tx_i+b)\ge a$，这是因为正确分类的时候，$y$与$w^Tx+b$同符号
要求在$w,b$约束的条件下使$\frac{a}{|w|}$最大，最大几何间断
可以定义$\widehat{w}=\frac{w}{a},\widehat{b}=\frac{b}{a}$，可有$(w,b)\rightarrow(\widehat{w},\widehat{b})$
故原来的优化目标可变为$\max_{\widehat{w},\widehat{b}}\frac{1}{|\widehat{w}|},s.t. y_i(\widehat{w}^Tx_i+\widehat{b})\ge 1$
由于a不影响符号，都线性可分，所以不影响分类性能
而$\max_{\widehat{w},\widehat{b}}\frac{1}{|\widehat{w}|}$等价于$\min_{\widehat{w},\widehat{b}}|\widehat{w}|$
故可有$\min_{\widehat{w},\widehat{b}}\frac{1}{2}|\widehat{w}|,s.t.y_i(\widehat{w}^Tx_i+\widehat{b})\ge 1,\forall i$，其中加$\frac{1}{2}$是为了便于计算的系数
<font color='red'><b>
将约束条件融入到优化目标函数中，建立拉格朗日公式
</font></b>
可有：
$L(\widehat{w},\widehat{b},\alpha)=\frac{1}{2}|\widehat{w}|^2-\sum_{i=1}^n\alpha_i(y_i(\widehat{w}^Tx_i+\widehat{b})-1),s.t.\alpha_i\ge 0$
其中$\alpha$是拉格朗日系数，$\alpha=[\alpha_1,...,\alpha_n]^T$
此时优化目标变为$\min_{\widehat{w},\widehat{b}}(\max_{\alpha_i\ge 0}L(\widehat{w},\widehat{b},\alpha))$
即表示$L$先对$\alpha$求最大，后对$\widehat{w},\widehat{b}$求最小
进行展开可有
$L(\widehat{w},\widehat{b},\alpha)=\frac{1}{2}|w|^2-\sum_{i=1}^n\alpha_i y_i \widehat{w}^Tx_i-\sum_{i=1}^n\alpha_i y_i \widehat{b}+\sum_{i=1}^n\alpha_i$
<font color='red'><b>
需要转换为其对偶问题来求解。原始问题通过满足KKT(<font size=4>原始变量求偏导，最大时约束部分为0</font>)，已转化为对偶问题，后让$L(\widehat{w},\widehat{b},\alpha)$关于$\widehat{w},\widehat{b}$最小化，后对$\alpha$求极大，后用SMO求解
</font></b>
即求解$\max_{\alpha_i\ge 0}(\min_{\widehat{w},\widehat{b}}L(\widehat{w},\widehat{b},\alpha))$
此时可以考虑KKT条件
即先固定$\alpha$，然后对$\widehat{w},\widehat{b}$求导来求最小
故可有：
$\left \{ \begin{array}{c}\frac{dL}{d\widehat{w}}=|\widehat{w}|-\sum_{i=1}^n\alpha_iy_ix_i=0\\\frac{dL}{d\widehat{b}}=-\sum_{i=1}^n\alpha_iy_i=0\\ \alpha_i(y_i(\widehat{w}^Tx_i+\widehat{b})-1)=0\end{array} \right.$
代入原式后可有
$\max_{\alpha_i\ge 0}w_{(\alpha)}=\max_{\alpha_i\ge 0}(\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1,j=1}^n\alpha_i \alpha_j y_i y_j x_j^Tx_i)$
转为$\min$为
$\min_{\alpha}(\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j y_i y_j x_i^Tx_j-\sum_{i=1}^n\alpha_i,s.t.\sum_{i=1}^n\alpha_i y_i=0)$
得到$\alpha_i$之后，可有
$\widehat{w}=\sum_{i=1}^n\alpha_i y_i x_i$
又因为KKT要求$\forall i \alpha_i(y_i(\widehat{w}^Tx_i+\widehat{b})-1)=0$
所以要么$\alpha_i=0$或$y_i(\widehat{w}^Tx_i+\widehat{b})=1$
将$\widehat{w}=\sum_{i=1}^n\alpha_i y_i x_i$代入，可有
$\widehat{b}=\frac{1}{y_i}-\widehat{w}^Tx_i=\frac{1}{y_i}-\sum_{j=1}^n\alpha_j y_j(x_j^Tx_i)$
因为$y_i\in\{-1,1\}$
故可以有$\widehat{b}=y_i-\sum_{j=1}^n\alpha_j y_j (x_j^Tx_i)$
此时预测函数为：
<b>
$\widehat{h}(x)=sign(\widehat{w}^Tx+\widehat{b})$
对任何一个样本$x_i$有
$\widehat{h}(x_i)=sign(\widehat{w}^Tx_i+\widehat{b})$
即
$\widehat{h}(x_i)=sign(\sum_{j=1}^n\alpha_j y_j x_j^Tx_i+y_i-\sum_{j=1}^n\alpha_j y_j (x_j^Tx_i))$
</b>
<font color='red'><b>
可以看到，此时的预测函数对于任何一个样本，用该样本来更新权重的时候，都需要和其他全部样本进行比较，相当于每次的输入是一个<font size=5>样本对</font>
</font></b>

### 线性不可分

线性不可分的时候，引入松弛变量$\epsilon_i$，表示容许某个样本违反超平面，即优化目标变为：
$\min_{\widehat{w},\widehat{b}}(\frac{1}{2}|w|^2+C\sum_{i=1}^n\epsilon_i),s.t.y_i(w^Tx_i+b)\ge1-\epsilon_i,\epsilon_i\ge 0$
$\epsilon_i$为误差(松弛变量)，缩小间距，C是折中参数
将约束日傲剑融合到优化目标中，有拉格朗日
$L(\widehat{w},\widehat{b},\alpha,\beta)=\frac{1}{2}|\widehat{w}|^2+C\sum_{i=1}^n\epsilon_i-\sum_{i=1}^n\alpha_i(y_i(\widehat{w}^Tx_i+\widehat{b})-1+\epsilon_i)-\sum_{i=1}^n\beta_i\epsilon_i$
其中$\alpha,\beta$为拉格朗日系数，$s.t.\alpha\ge 0,\beta\ge 0,\epsilon_i\ge 0$
此时优化目标为
$\min_{\widehat{w},\widehat{b},\epsilon}(\max_{\alpha,\beta}(L(\widehat{w},\widehat{b},\epsilon,\alpha,\beta)))$
即对$\alpha,\beta$取最大，之后对$\widehat{w},\widehat{b},\epsilon$取最小
考虑KKT：
$\left \{ \begin{array}{c}\frac{dL}{d\widehat{w}}=|w|-\sum_{i=1}^n\alpha_i y_i x_i=0\\ \frac{dL}{d\widehat{b}}=\sum_{i=1}^n\alpha_i y_i=0\\ \frac{dL}{d\epsilon_i}=C-\alpha_i-\beta_i=0,注意这里是对某一个\epsilon_i求导 \\ \alpha_i(y_i(\widehat{w}^Tx_i+\widehat{b})-1+\epsilon_i)=0 \\ \beta_i\epsilon_i=0 \end{array} \right.$
由以上公式，可转化为对偶问题
$\max_{\alpha,\beta}(\min_{\widehat{w},\widehat{b},\epsilon}(L(\widehat{w},\widehat{b},\epsilon,\alpha,\beta)))$
代入原式可有
$L(\widehat{w},\widehat{b},\epsilon,\alpha,\beta)=-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j y_i y_j x_i^Tx_j+\sum_{i=1}^n\alpha_i$
对$\alpha_i$求极大，可求得$\widehat{w},\widehat{b}$
$\widehat{w}=\sum_{i=1}^n\alpha_i y_i x_i,\widehat{b}=(1-\epsilon_i)y_i-\sum_{i=1}^n\alpha_i y_i x_i^Tx_j$
<font color='red'><b>
可以看到，仍然可以用<font size=5>样本对</font>来解决
</font></b>

## 核函数

采用样本对时就可以考虑使用核函数。
<b>
核函数可以处理即便引入松弛变量也不能处理的非线性问题，即升维到高维空间
</b>
特征映射：将数据从原始空间映射到高维空间的函数，如极坐标/二次多项式
决策函数：$h(x)=\widehat{w}^Tx+\widehat{b}\Longrightarrow h'(x)=\widehat{w}^T\phi(x)+\widehat{b}$
优化目标：$\min_{\widehat{w},\widehat{b},\epsilon} \frac{1}{2}|\widehat{w}|^2+C\sum_{i=1}^n\epsilon_i,s.t.y_i(\widehat{w}^T\phi(x_i)+\widehat{b})\ge1-\epsilon_i,\forall i$
当不知道特征映射的时候，就不可解
重点在于如何计算特征映射以及如何找到合适的特征映射
<font color='red'><b>
核函数：样本在特征空间中（特征映射决定的空间）的内积，即
$k(x_i,x_j)=<\phi(x_i),\phi(x_j)>=\phi(x_i)^T\phi(x_j)$
</font></b>
核函数会是一个矩阵，样本的内积矩阵
此时优化目标进行拉格朗日
$L(\widehat{w},\widehat{b},\epsilon,\alpha,\beta)=\frac{1}{2}|w|^2+C\sum_{i=1}^n\epsilon_i-\sum_{i=1}^n\alpha_i(y_i(\widehat{w}^T\phi(x_i)+\widehat{b})-1+\epsilon_i)$
其中$\alpha,\beta$是拉格朗日乘子
有KKT，即
$\left \{ \begin{array}{c}\frac{dL}{d\widehat{w}}=0\\ \frac{dL}{d\widehat{b}}=0\\ \frac{dL}{d\epsilon_i}=0\\ \alpha_i(y_i(\widehat{w}^T\phi(x_i)+\widehat{b})-1+\epsilon_i)=0\\ \beta_i\epsilon_i=0\end{array} \right .$
优化目标为
$\min_{\alpha,\beta(\beta可删去)}(\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j y_i y_j \phi(x_i)^T\phi(x_J))-\sum_{i=1}^n\alpha_i$
$s.t.\sum_{i=1}^n\alpha_i y_i=0,C\ge\alpha_i\ge 0$
即
$\min_{\alpha}(\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j y_i y_j k(x_i,x_j))-\sum_{i=1}^n\alpha_i$
$s.t.\sum_{i=1}^n\alpha_i y_i=0,C\ge\alpha_i\ge 0$
其中$\alpha$只与样本数n有关，与样本维度无关
通过选好的特征映射，找到好的核函数
<font color='red'>
对一个征订核函数，特征映射必然存在，保证正定，不一定唯一
正定函数：处零点外恒为正值的标量函数
</font>

