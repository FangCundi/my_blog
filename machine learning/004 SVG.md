# SGD 随机梯度下降

## base

如何快速找到最优解(迭代，从一个初始解开始，向正确方向努力)
<b>
即$k_{t+1}=k_t+\alpha g(k_t)$
其中$\alpha$为步长，$g(k_t)$为下降方向
</b>
$g(k_t)$条件：$\left \{ \begin{array}{} g(k_t)要让k_{t+1}向最优解逼近(判定)\\ k_t达到最优解时g(k_t)=0 \end{array} \right .$
即表示当$k_{t+1}=k_t$时有$g(k_t)=0$
核心问题：找一个满足条件的$g(k_t)$，这里选择的是函数$f(k)$的梯度
梯度优化：方向+步长，步长越大，可能发生徘徊；步长越小，找的越慢
大规模梯度优化中计算开销打，时间复杂度高

## 梯度下降法（针对可微函数）

$d f(w)=(\frac{d f(w)}{d w_1},\frac{d f(w)}{d w_2},...,\frac{d f(w)}{d w_d})^T$
要求$w_{t+1}=w_t-\eta df(w_t)$，用$-\eta$为的是梯度的反方向

### 求证收敛，即向结果偏移

<b>

求证$f(w_{t+1})\le f(w_t)$

</b>

因为$f(w_{t+1})=f(w_t-\eta df(w_t))$
进行泰勒展开可有
$f(w_{t+1})=f(w_t)+(w_t-\eta df(w_t)-w_t)^Td f(w_t)$
可有$f(w_{t+1})=f(w_t)-\eta df(w^t)^Td f(w_t)$
即$f(w_{t+1})=f(w_t)-\eta|d f(w_t)|^2,\eta>0$
因为$\eta|d f(w_t)|^2\ge 0$
故$f(w_{t+1})\le f(w_t)$

### 求证上限，即迭代上线

假设$|w_*|$为最优解，且$|w_*|\le B$
假设输出的权重$\widehat{w}$是当前T次迭代的所有权证权重的平均值，即$\widehat{w}=\frac{1}{T}\sum_{t=1}^Tw_t$
要求其有迭代上界，即找到输出权重找到的接与最优权重找到的解的差异，即$f(\widehat{w})-f(w_*)$的差异
<b>
这里是由于凸函数的性质，沿着梯度下降的方向迭代，偏差应该是下降的。
可以把$w_*$理解为最优解，$\widehat{w}$是当前解，$T$为迭代次数
</b>
$f(\widehat{w})-f(w_*)\le \frac{1}{T}\sum_{t=1}^T(f(w_t)-f(w_*))$
因为微分的时候，可以有$f(w_t)-f(w_*)\le <w_t-w_*,d f(w_t)>$
即梯度乘x的长度大于等于y的长度，之前已经证明了梯度会减小
又因为$w_{t+1}=w_t-\eta v_t,v_t=d f(w_t),\eta$为步长
代入可以有
$f(\widehat{w})-f(w_*)\le \frac{1}{T}\sum_{t=1}^T(f(w_t)-f(w_*))$
即
$f(\widehat{w})-f(w_*)\le \frac{1}{T}\sum_{t=1}^T(<w_t-w_*,d f(w_t)>)$
即
$f(\widehat{w})-f(w_*)\le \frac{1}{T}\sum_{t=1}^T(<w_t-w_*,v_t>)$
又因为可以乘一个$\eta$再除以一个$\eta$
$<w_t,w_*,v_t>=\frac{1}{\eta}<w_t-w_*,\eta v_t>$
因为矩阵乘法可以有：
$x^Ty=-\frac{1}{2}(|x-y|^2-|x|^2-|y|^2)$
故可有
$\frac{1}{\eta}<w_t-w_*,\eta v_t>=-\frac{1}{2\eta}(|w_t-w_*-\eta v_t|^2-|w_t-w_*|^2-|\eta v_t|^2)$
因为$w_{t+1}=w_t+\eta v_t$
代入可有
$\frac{1}{\eta}<w_t-w_*,\eta v_t>=-\frac{1}{2\eta}(|w_{t+1}-w_*|^2-|w_t-w_*|^2-|\eta v_t|^2)$
故可有，求和展开后
$\sum_{t=1}^T(<w_t-w_*,v_t>)=-\frac{1}{2\eta}(|w_{t+1}-w_*|^2-|w_1-w_*|^2)+\frac{\eta}{2}\sum_{t=1}^T|v_t|^2$
因为正常运行时，$w_{t+1}$很接近$w_*$，故可以近似
$\sum_{t=1}^T(<w_t-w_*,v_t>)\le \frac{1}{2\eta}(|w_1-w_*|^2)+\frac{\eta}{2}\sum_{t=1}^T|v_t|^2$
而因为$w_1$是初识给定的初始解$v$
故可有
$\sum_{t=1}^T(<w_t-w_*,v_t>)\le \frac{|w_*|^2}{2\eta}+\frac{\eta}{2}\sum_{t=1}^T|v_t|^2$
而之前给了一个上界最小值$B$，这里可以假设一个$\rho$，使
$|w_*|\le B,|v_t|\le \rho$
这里的$\rho$表示导数的上界，或者说梯度的上界
故有
$\sum_{t=1}^T(<w_t-w_*,v_t>)\le \frac{B^2}{2\eta}+\frac{\eta T}{2}\rho^2$
则可有
$\frac{1}{T}\sum_{t=1}^T(<w_t-w_*,v_t>)\le \frac{B^2+\eta^2T\rho^2}{2\eta T}$
其中只有一个变量$\eta$，即我们人为给定的步长，对其求导可以得到步长的最优解
$\eta=\sqrt{\frac{B^2}{\rho^2-T}}$
代入可有
$\frac{1}{T}\sum_{t=1}^T(<w_t-w_*,v_t>)\le \frac{BP}{\sqrt{T}}$
即
$f(\widehat{w})-f(w_*)\le \frac{BP}{\sqrt{T}}$
即误差有一个上界，当误差给定的时候，即
$f(\widehat{w})-f(w_*)\le \epsilon$，即要求与期望的结果相差小于某个给定的参数的时候，可有
$T\ge \frac{B^2P^2}{\epsilon^2}$
此时$T$为迭代次数/收敛速率，表示至少这么多次才能将误差降低到$\epsilon$，记为
$O(\frac{1}{\epsilon})$
比较慢，$O(\frac{1}{\epsilon})$比$O(\frac{1}{\sqrt{\epsilon}})$更快

## 子（次）梯度下降

函数不可微时，如何找下降方向
$\forall u,f(u)\ge f(w)+<u-w,d f(w)>$
只要满足上面的条件即可，即
$\forall u,f(u)\ge f(w)+<u-w,v>,v$是次梯度
含义是，对任意向量$v$，对任意的权重$u$都有上式成立，则$v$是在$u$点的次梯度

### 例：求$|x|$的次梯度

$|x| \left\{ \begin{array}{} 1&x>0\\-1&x<0\\?&x=0 \end{array} \right.$
令任意$u$满足$\forall u,|u|\ge |w|+<u-w,v>$
可有$|u|\ge <u,v>$
即$|u|\ge uv\Longrightarrow v\in[-1,1]$
故可有
$|x| \left\{ \begin{array}{} 1&x>0\\-1&x<0\\v\in[-1,1]&x=0 \end{array} \right.$

### 例：求$f(w)=\max\{0,1-y<w,x>\}$的次梯度

可给定$g_i(w)$均为凸函数，则可有
$d g_j(w)\in d g(w),j\in \argmax g_i(w)$
即$\forall i,g_i(w^Tx+b)\ge 1-\epsilon_i\Longrightarrow \epsilon_i\ge 1-g_i(w^Tx_i+b)$
<font color='red'><b>
个人理解为：当某一个点可以有多个函数表示的时候，选择其中最大的作为次梯度
</font></b>
故当$f(w)=\max\{0,1-y<w,x>\}$时，他需要进行分段
$\left \{ \begin{array}{} -y_ix_i & 1-y_i<w,x_i>> 0\\Co\{-y_ix_i,0\}&1-y_i<w,x_i>=0\\0&other \end{array}\right .$
<font color='red'><b>注意，第2个情况中是向量的凸包，是一个集合</font></b>

## SGD随机梯度下降

不要求下降方向是梯度方向，只需要每次下降一点点
$v$可以使随机变量，但要求$E(v)$必须是当前位置的次梯度里面的一个
一般不能求接待约束的问题，因为要求计算梯度后还要满足约束
$\eta>0,T>0,w=0$初始化

```python
for t=1,...,T:阈值
    选一个vt，要求E(vt)在次梯度中
    更新wt+1=wt-ηvt
```

输出$\widehat{w}=\frac{1}{T}\sum_{t=1}^Tw_t$

### 例：$\min_{w,\epsilon}\frac{\lambda}{2}|w|^2+\sum_{i=1}^n\epsilon_i,s.t.y_i(w^Tx_i)\ge 1-\epsilon_i,\epsilon_i\ge 0$

首先转化为无约束
$\min_{w,\epsilon}\frac{\lambda}{2}|w|^2+\sum_{i=1}^n\max\{0,1-y_i(w^Tx_i)\}$
这里是根据$\epsilon_i\ge 1-y_i(w^Tx_i)$得到的，个人感觉类似于拉格朗日方程的求法
然后观察，对于函数$f(w)=\frac{\lambda}{2}|w|^2+\sum_{i=1}^n\max\{0,1-y_i(w^Tx_i)$
前半部分是可微的$\frac{\lambda}{2}|w|^2$，后半部分是不可微的，需要用次梯度
求导可有$\frac{dw_t}{df}=\lambda w_t+v_t$
其中$v_t$就是次梯度
所以针对任意一个点，可有
$w_{t+1}=w_t-\frac{1}{\lambda t}(\lambda w_t+v_t)$
这里我们人为设定步长$\eta=\frac{1}{\lambda t}$，感觉只是为了简便计算，对结果无影响。
将上式拆开可有
$w_{t+1}=w_t-\frac{1}{\lambda t}\lambda w_t-\frac{1}{\lambda t}v_t$
即
$w_{t+1}=\frac{t-1}{t}w_t-\frac{1}{\lambda t}v_t$
将$w_t=w_{t-1}-\eta(\lambda w_{t-1}+v_{t-1})$代入可有
$w_{t+1}=\frac{t-1}{t}(w_{t-1}-\eta(\lambda w_{t-1}+v_{t-1}))-\frac{1}{\lambda t}v_t$
即$w_{t+1}=\frac{t-1}{t}(\frac{t-2}{t-1}w_{t-1}-\frac{1}{\lambda(t-1)}v_{t-1})-\frac{1}{\lambda t}v_t$
由此，可以一直推导到$w_0=0$初始解，可有
$w_{t+1}=-\frac{1}{\lambda t}\sum_{i=1}^tv_i$
即某一点的下一个迭代的权重$w_{t+1}$与之前的权重无关，只与每一次迭代的次梯度有关。
而选择步长为$\eta=\frac{1}{\lambda t}$只是为了约去每一次迭代的权重$w_i$

当约束条件拿不进去的时候，就进行缩放来解决
